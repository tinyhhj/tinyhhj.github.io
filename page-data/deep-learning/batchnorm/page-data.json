{"componentChunkName":"component---src-templates-blog-post-js","path":"/deep-learning/batchnorm/","result":{"data":{"site":{"id":"Site","siteMetadata":{"title":"tinyhhj Blog"}},"markdownRemark":{"id":"d8dd060f-8b29-5d6a-802b-a7d959c1285c","excerpt":"Batch norm…","html":"<h2>Batch norm 효과</h2>\n<ol>\n<li>\n<p>문제  </p>\n<ul>\n<li>이상적으로는 모든 데이터를 학습에 사용할 수 있다면 완벽한 학습모델이 가능하지만 현실은 그럴 수 없기때문에 한정된 데이터에 대해서 학습을 시킬 수 밖에없다.</li>\n<li>한정된 데이터로 학습을 시키기 때문에, 새로운 데이터의 존재에 대해서 좋은 결과를 추출해내기 위해서 <code class=\"language-text\">모델의 일반화</code>가 필수적이다.</li>\n<li>데이터 분포의 변화는(새로운 데이터 출현) <code class=\"language-text\">공변량 변화</code>라고도 지칭하는데, 이 변화는 관측함수의 변화를 이끌어낸다.(성능이 좋은쪽으로든 나쁜쪽으로든)</li>\n</ul>\n</li>\n<li>batch norm의 역할<br>\n5 레이어 네트워크라고 가정을 할때, 3번째 레이어의 경우에 앞의 2번째 레이어의 결과를 관측값 y로 매핑하는 학습을 진행중이다. 하지만, 입력값과 1,2번째 레이어의 학습 매개변수(w,b)값이 변경되면서 3번째 레이어의 입장에서는 매번 입력 데이터 분포의 변화 즉 <code class=\"language-text\">공변량의 변화</code>를 겪게 되는것이다.<br>\n따라서 입력값 데이터 분포에 대해서 학습 영향을 최소로 하고자, batch norm을 통해서 3번째 입력 값의 평균과 분산을 0과 1 혹은(a,b)처럼 고정시켜 최대한 일반화된 모델로 학습을 유도하는 것이다. 또한 이전 레이어가 학습을 하면서 값을 변경시키는 부분을 완화시켜 레이어 사이에 의존성을 없애고, 독립적으로 학습이 가능하도록 돕는다.</li>\n</ol>\n<p>또한, batch norm은 미니배치에 대해서 평균, 분산을 구하므로 dropout과 같이 히든 레이어에 약간의 노이즈를 입히는 역할을 하여 민감한 모델(오버피팅)이 되지 않도록 돕는 역할을 한다.</p>\n<h3>reference</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=nUUqwaxLnWs\">youtube</a></li>\n</ul>","frontmatter":{"title":"[deeplearning]batchnorm","date":"February 23, 2020","description":"deep-learning"}}},"pageContext":{"slug":"/deep-learning/batchnorm/","previous":{"fields":{"slug":"/deep-learning/blank-review/simple-classifier/"},"frontmatter":{"title":"[deeplearning]simple-classifier"}},"next":{"fields":{"slug":"/python/relative-import/"},"frontmatter":{"title":"[python]relative-import"}}}}}
{"componentChunkName":"component---src-templates-blog-post-js","path":"/deep-learning-from-scratch/index3/","result":{"data":{"site":{"id":"Site","siteMetadata":{"title":"Gatsby Starter Blog"}},"markdownRemark":{"id":"7ac581ee-36e9-5d9a-b3b7-58a2e1b66c94","excerpt":"deep-learning-from-scratch chap 3","html":"<h2><a href=\"https://github.com/WegraLee/deep-learning-from-scratch\">deep-learning-from-scratch</a></h2>\n<h3>chap 3</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&#39;&#39;&#39;\n신경망\n데이터로부터 가중치 매개변수의 적절한 값을 자동으로 학습하는 능력이 신경망의 중요한 성질입니다.\n신경망의 구조\n0.입력층\n1.은닉층\n2.출력층\n\n퍼셉트론의 편향은 입력이 1이고 가중치가 b일 경우의 가중치로 변경이 가능합니다.\na = b(=w0)*x0 +w1*x1 + .. + wn*xn 으로 표현이 가능합니다.\ny = h(a)\n여기서 h(a)는 활성화 함수라고 하는데 입력의 총합(a)이 활성화(y)를 일으키는지 정하는 역할을 합니다.\n\n퍼셉트론의 경우에 활성화 함수로 계단 함수를 사용하고 있으나 신경망으로 세계에서는 활성화 함수로 다른 함수를 사용하고 있습니다.\n\n시그모이드 함수\n\n&#39;&#39;&#39;\n\n# 계단 함수\nimport numpy as np\nimport matplotlib.pylab as plt\n\ndef step_func(x1):\n    return np.array( x1 &gt;0, dtype=np.int)\n\nx = np.arange(-5., 5.,0.1)\ny = step_func(x)\nplt.plot(x,y)\nplt.ylim(-0.1, 1.1) # 범위\nplt.show()\n\n# sigmoid\n\ndef sigmoid(x):\n    # broadcast 이용\n    # scalar 와 np.array 연산 시 해당 크기만큼 scalar값이 확장되어 연산을 수행합니다.\n    return 1 / (1 + np.exp(-x))\n\nxx = np.arange(-5.,5.,0.1)\nyy = sigmoid(xx)\n\nplt.plot(xx,yy)\nplt.ylim(-0.1,1.1)\nplt.show()\n\n# ReLU(Rectified Linear Unit)\n\ndef relu(x):\n    return np.maximum(0,x)\n\n# 다차원 배열\nA = np.array([[1,2],[3,4]])\nprint(A)\nprint(np.ndim(A)) # 차원\nprint(A.shape) # 차원의 크기\n\nB = np.array([[4,5],[7,8]])\nprint(np.dot(A,B)) # 행렬의 곱 np.dot\n\nC = np.array([[[1],[3]],[[1],[3]],[[1],[3]]])\nprint(C.shape)\n\n# 신경망 행렬 곱\n# x * w = y\n\nX = [1,2]\nW = [[1,2,3],[4,5,6]]\nY = np.dot(X,W)\nprint(Y)\n\n# 핵심은 신경망에서의 계산을 행렬 계산으로 정리할 수 있다는 것입니다.\n# 0층 입력층\n# 1층 은닉층\n# 2층 은닉층\n# 3층 출력층\nX = np.array([0.5,1])\nW1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\nB1 = np.array([0.1,0.2,0.3])\nprint(W1.shape)\nprint(X.shape)\nprint(B1.shape)\n\n# 0-&gt;1층\nA1 = np.dot(X,W1)+B1\nZ1 = sigmoid(A1)\nprint(A1)\nprint(Z1)\n\nW2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\nB2 = np.array([0.1,0.2])\n\nprint(Z1.shape)\nprint(W2.shape)\nprint(B2.shape)\n\n# 1-&gt;2층\nA2 = np.dot(Z1,W2) + B2\nZ2 = sigmoid(A2)\nprint(A2)\nprint(Z2)\n\n# 2-&gt;3층\n# 나머지 구현은 같으나, 활성화 함수만 다릅니다.\ndef identify_func(x):\n    return x\n\nW3 = np.array([[0.1,0.3],[0.2,0.4]])\nB3 = np.array([0.1,0.2])\nA3 = np.dot(Z2,W3)+B3\nZ3 = identify_func(A3)\nprint(A3)\nprint(Z3)\n\n# 중요: 출력층의 활성화 함수는 문제의 성질에 다르게 정합니다\n# 회귀: 항등함수\n# 분류: 시그모이드\n# 다중분류: 소프트 맥스\n\n# 소프트 맥스\n\na = np.array([0.3, 0.5, 0.7])\nexp_a = np.exp(a)\nsum_exp_a = np.sum(exp_a)\ny = exp_a / sum_exp_a\nprint(y)\n\n# softmax의 결과값의 경우 항상 출력 총합이 1 이므로 출력값을 &#39;확률&#39;로 해석할 수 있습니다.\ndef softmax(x):\n    # exp는 오버플로우가 발생할 수 있으므로 max값을 빼줍니다.\n    max = np.max(x)\n    exp_x = np.exp(x-max)\n    sum = np.sum(exp_x)\n    # print(max, exp_x, sum)\n    return exp_x / sum\n\n# 기계학습을 학습과 추론 2단계로 나뉩니다\n# 학습: 데이터를 사용해서 &#39;가중치 매개변수&#39;를 학습합니다.\n# 추론: 앞에서 학습한 &#39;가중치 매개변수&#39;를 사용해서 입력데이터를 분류합니다.\nimport sys, os\nsys.path.append(os.pardir)\nfrom download_mnist import load_mnist\n\n(x_train, t_train), (x_test,t_test) = load_mnist(flatten=True, normalize=True)\n\nprint(x_train.shape)\nprint(t_train.shape)\nprint(x_test.shape)\nprint(t_test.shape)\n\n# 출력층 뉴런 10개 -&gt; 다중 분류 0-9(10개\n# 은닉 1층 50개 뉴런 / 2층 100 개 뉴런\n# 50 과 100 은 임의의 값\nimport pickle\n\ndef get_data():\n    (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=True)\n    return x_test,t_test\n\ndef init_network():\n    with open(&quot;sample_weight.pkl&quot;,&#39;rb&#39;) as f:\n        network = pickle.load(f)\n\n    return network\n\ndef predict(network, x):\n    W1,W2,W3 = network[&#39;W1&#39;], network[&#39;W2&#39;],network[&#39;W3&#39;]\n    b1,b2,b3 = network[&#39;b1&#39;],network[&#39;b2&#39;],network[&#39;b3&#39;]\n\n    a1 = np.dot(x,W1)+b1\n    z1 = sigmoid(a1)\n    a2 = np.dot(z1,W2)+b2\n    z2 = sigmoid(a2)\n    a3 = np.dot(z2,W3)+b3\n    y = softmax(a3)\n\n    print(&#39;a1.shape&#39;, a1.shape)\n    print(&#39;z1.shape&#39;, z1.shape)\n    print(&#39;a2.shape&#39;, a2.shape)\n    print(&#39;z2.shape&#39;, z2.shape)\n    print(&#39;a3.shape&#39;, a3.shape)\n    print(&#39;y.shape&#39;, y.shape)\n    return y\n\nx, t = get_data()\nnetwork = init_network()\n\nbatch_size = 100\naccuracy_cnt = 0\nfor i in range(0,len(x), batch_size):\n    x_batch = x[i:i+batch_size]\n    y_batch = predict(network, x_batch)\n    #y = predict(network, x[i])\n    p = np.argmax(y_batch, axis=1)\n    accuracy_cnt+= np.sum(p == t[i:i+batch_size])\n\nprint(&quot;Accuracy: &quot; + str(float(accuracy_cnt) / len(x)))</code></pre></div>","frontmatter":{"title":"[python]deep-learning-3","date":"January 28, 2020","description":"deep-learning"}}},"pageContext":{"slug":"/deep-learning-from-scratch/index3/","previous":{"fields":{"slug":"/algospot-quadtree/"},"frontmatter":{"title":"[algospot]quadtree"}},"next":{"fields":{"slug":"/deep-learning-from-scratch/index2/"},"frontmatter":{"title":"[python]deep-learning-2"}}}}}